{"cells":[{"cell_type":"markdown","source":["#Credit Card Fraud Detection"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dd3bae2-3a64-4f3f-a910-fd17070cd4df"}}},{"cell_type":"markdown","source":["**Data**: The dataset covers credit card transactions done by European cardholders in September 2013.\nIn this dataset, we have 492 frauds out of 284,807 transactions that happened in two days. The dataset is heavily skewed, with the positive class (frauds) accounting for just 0.172 percent of all transactions. It contains only PCA transformation result as numerical input variables. The major components derived with PCA are features V1, V2,... V28; the only features not changed with PCA are 'Time' and 'Amount.' The seconds elapsed between each transaction and the first transaction in the dataset are stored in the 'Time' field. The transaction Amount is represented by the feature 'Amount'. The feature 'Class' has a value of 1 when there is a fraud and 0 when there isn't. This dataset is from [Kaggle- credit card fraud detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).\n             \n**Goal**: The goal of this approach is to maintain or improve the performance of classifiers in the context of excessively imbalanced and large datasets using Big Data techniques.\n\n**Approach**: Local \n\n**Steps**: 1. Divide data into Clusters \n           2. Repartion of data based on clusters\n           3. SMOTE+ENN on each partition - using Map reduce\n           4. Reduce the data to sparkDf\n           5. Fit Classifier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"303b1b35-26d8-45e7-aee5-101afb70f8bd"}}},{"cell_type":"code","source":["sqlContext.clearCache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0aae934f-673d-4027-83b1-51287ea6d838"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Import required packages and libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54c0937a-261e-4759-8f20-3d565cc53288"}}},{"cell_type":"code","source":["# Import all the required libraries\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e001445e-1a00-46b2-ada1-f2a29ed773b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Read and understand the data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91703c3f-55e9-4154-9ed6-55ef6112fea2"}}},{"cell_type":"code","source":["# Load the dataset which is in Comma-Separated Value (CSV) format.\ndf = spark.read.option(\"header\",True).csv(\"/mnt/sjjhkkdf/creditcard.csv\")\ndf=df.drop(\"Time\")\n  \n# Cache DataFrame so that we only read it from disk once\ndf.cache()\n\n#Check the name, datatype and values of columns\ndf.take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5571bd4c-22e7-421c-8d00-fe11ad5318ec"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44dd4c29-8100-41d0-ab17-90f11f3454ac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["####Data description\n**Attribute information**:\n```\nThe features V1, V2,... V28 are derived from PCA transformation.\n\n'Time'- The seconds elapsed between each transaction and the first transaction in the dataset \n\n'Amount'- The transaction Amount \n\n'Class'- a value of 1 when there is a fraud and 0 when there isn't.\n\n```\n**The target variable is the class of the transaction i.e. fraud (1) or not (0).**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62f454d7-d06e-4530-9aea-7d6b8ad1edd7"}}},{"cell_type":"markdown","source":["##Data preparation\n\nAs all the features are in string, cast all features to floats except Time and Class"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90f054a7-5800-4103-954e-8fe30607437b"}}},{"cell_type":"code","source":["#get the list of name of the columns except 'Time' and 'Class'\ntargets = [i for i in df.schema.names if i not in [\"Class\"]]\n# \"Time\",\nfor col in targets: \n  df = df.withColumn(col, df['`{}`'.format(col)].cast('float'))\n\n#Cast 'Time' and 'Class' to integer\nfor col in [\"Class\"]:\n  df = df.withColumn(col, df['`{}`'.format(col)].cast('integer'))\n\n# \"Time\",\n#Check the datatype of columns\ndf.printSchema()\n\n#Visualize the data\n# display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67fbb884-6dc5-40ae-935f-63be6fee6cc5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Data Visualization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52696885-4283-480b-bae3-b8ce67797f75"}}},{"cell_type":"markdown","source":["#### Histogram of classes\nTotal fraud transactions in the dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33deeb38-5389-4f1c-9556-8dc6bd61a3e2"}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\ndf_count=df.withColumn(\"Class\",df.Class.cast('string')).groupby('Class').count().take(2)\nprint(df_count)\n(x_values, y_values) = zip(*df_count)\nplt.bar(x_values, y_values)\nplt.title('Histogram of number of non-fraudulent and fradulent transactions')\nplt.xlabel('Transactions class')\nplt.ylabel('Frequency')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2107a8ba-67e5-457f-9e27-2311a9478e44"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_min = df.filter(df.Class == 1)\ndf_new = df.filter(df.Class == 0).sample(0.3)\n\nun = df_min.union(df_new)\n\n# un.filter(df_new.Class == 0).count()\n\n# df_train, df_test = un.randomSplit([0.7,0.3], seed=1)\n\ndf_train, df_test = df.randomSplit([0.7,0.3], seed=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11553412-3c77-46e3-9182-1df6cfb220d6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Create Vector Assembler"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38a20e18-7567-4e40-892d-874f20db0835"}}},{"cell_type":"code","source":["# Note there are two differnt API's for kmeans one for dataframes and another for RDD's\n# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html\n\n# Convert features into a vector column\nassembler = VectorAssembler(\n  inputCols= [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"], \n  outputCol= \"features\"\n)\n\noutput = assembler.transform(df_train)\nprint(\"Assembled predictor columns to vector column 'features'\")\noutput.select(\"features\", \"Class\").show(truncate=True)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28a6ec82-5476-442c-86b2-aac714be5523"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Clustering\nCreate clusters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11682399-1bda-47ac-93d1-b68681c27391"}}},{"cell_type":"code","source":["df_train.drop(\"time\")\na = list(df_train.columns)\n# df_train_cluster = pre_smote_df_process(df_train, a, cat_cols=[], target_col = \"Class\" )\n# Apply KMeans clustering using same seed value\nimport time\nstart_time=time.time()\n\n\nkmeans = KMeans().setK(6).setSeed(1)\\\n        .setFeaturesCol(\"features\")\\\n        .setPredictionCol(\"Cluster_no\")\nkmeansModel1 = kmeans.fit(output)        \ndf_clusters1=kmeansModel1.transform(output)\n# df_clusters1.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82e4aee0-d0f2-4fc8-b7e1-c8cf12bc062e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# drop 'features' column \ndf_clusters1 = df_clusters1.drop(\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca3082d0-4ea4-4369-bf71-6544d7cf08a7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Get the required number of partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"760efbd1-a64a-48ef-9e60-5884456567f4"}}},{"cell_type":"code","source":["# df_cluster_sub,df_cluster_sub_test= df_clusters1.randomSplit([0.5,0.5], seed=1)\n\n# df_clusters_partition=df_cluster_sub.sort(df_cluster_sub.Cluster_no.desc())\n# type(df_clusters_partition)\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import DataFrameWriter\n# df_clusters_partition.repartition(1)\n\nrequired_partitions = df_clusters1.select('Cluster_no').distinct().count()\nprint(required_partitions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10e549fc-c837-40a9-816e-c78437ca354a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Repartition of data based on the clusters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc6780c4-b25a-4498-ad22-2e538de303c7"}}},{"cell_type":"code","source":["def partitioner(x):\n  return int(x[0])\n\n\ndf_clusters_partition=df_clusters1.withColumn(\"Cluster_no\",df_clusters1.Cluster_no.cast('string'))\ndf_clusters_partition=df_clusters_partition.repartition(required_partitions)\ndf_clusters_partition=df_clusters_partition.rdd.map(lambda x: (x[\"Cluster_no\"],x)).partitionBy(required_partitions,partitioner).map(lambda x: x[1])\n# .partitionBy(required_partitions,partitioner)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37f6a80d-673d-4b08-8237-2c11653075ac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_clusters_partition.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fc2686f-5a07-4de0-ae94-740d73707b42"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Convert rdd to dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2fdc5e3-6393-4349-a692-f4e92ac2fdb3"}}},{"cell_type":"code","source":["df_partition=df_clusters_partition.toDF()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4311a211-9304-4c35-8581-a56a344baf9a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_partition.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a66c5a36-22f4-4b23-82a7-4c6a5fce3cab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# df_partition.select(spark_partition_id().alias(\"partitionId\")).groupBy(\"partitionid\").count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8987eff-01d4-4a3e-b6d7-2d0cb54182f3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# countInClusters=df_clusters1.groupBy(\"Cluster_no\").count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b17472b-16e6-4d67-b251-9c48ad286e4d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# countInClusters.sort(countInClusters.Cluster_no.desc()).show(400)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18b111c4-7cfa-4045-a04f-f1320f177f71"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_partition.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1292ada4-e406-4c6f-be25-ac987ac4db38"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Get the column names"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a62aa13c-f348-4202-beff-8ef358f73759"}}},{"cell_type":"code","source":["column_names=df_partition.columns\ncolumn_names.remove(\"Cluster_no\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b6ee600-7daf-468e-978a-c4f440865802"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## SMOTE+ENN"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57df511d-7543-46a0-8785-f825f452818a"}}},{"cell_type":"markdown","source":["### Apply SMOTEENN on each partition"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f820972a-1a95-4d87-9cc6-e023d73b5b87"}}},{"cell_type":"code","source":["\ndef toPandas_partition(instances):\n    import pandas as pd\n    panda_df = pd.DataFrame(columns = column_names)      #Â using the global variable\n    \n    for instance in instances:  # each instance is of Row type\n        dict_row=instance.asDict()\n#         del dict_row['Cluster_no']\n        panda_df = panda_df.append(dict_row, ignore_index=True)\n    return panda_df\n  \ndef self_training(partition):\n    import pandas as pd\n    import numpy as np\n    from pyspark.sql import Row\n\n    lab = toPandas_partition(partition)\n     \n    X = lab.loc[:,'V1':'Amount']\n    Y = lab.loc[:,\"Class\"]\n    Y=Y.astype('int')\n     \n    if len(Y)!=sum(Y): ratio= sum(Y)/(len(Y)-sum(Y))\n    else: ratio=0\n    \n    from imblearn.combine import SMOTEENN\n    from imblearn.over_sampling import SMOTE\n    \n    if ratio>0 and sum(Y)>5 and ratio<1:\n      sm2=SMOTEENN()\n      X_res, y_res = sm2.fit_resample(X,Y)\n      X_res=X_res.join(y_res)\n      return [X_res]\n    else:\n      return [lab]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d5c9700-5972-495e-a2b5-5fbf0f90372a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf_partition=df_partition.drop(col(\"Cluster_no\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8264163a-a0fc-45fc-b706-dd423595bff4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_partition.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"197cb8ee-5b1f-45da-9755-fe0a6842ec0d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_final=df_partition.rdd.mapPartitions(self_training).map(lambda x: (1,x)).reduceByKey(lambda x,y: x.append(y))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63b3fb8b-d808-42f1-8050-7974a33521e4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["final=df_final.take(1)[0][1]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af69cf81-194d-4d3f-bbd7-43dc11b450aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["final"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dbc1c1c-5bcf-41a9-9711-64e6fde4f41e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Check the 'Class' values after SMOTEENN"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51cb8f74-987a-488c-b652-8215eb83a590"}}},{"cell_type":"code","source":["final['Class'].value_counts()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66eac727-3c9b-490c-8b31-03fa1cd467fe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Convert the pandaDF to sparkDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e746d5c-0c3d-4c4a-9907-bb6ecb0b0731"}}},{"cell_type":"code","source":["from pyspark import SparkContext\nfrom pyspark.sql import SQLContext \nfrom pyspark.sql import SparkSession\n#Create PySpark SQL context\nsql = SQLContext(sc)\n\n#Create PySpark DataFrame from Pandas\nfinalDF=sql.createDataFrame(final)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3cd55e9-c396-4a9d-bd6a-abdf312a8281"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["end_time=time.time()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73f69b0a-6d40-41fd-896c-4ab30c652885"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Training"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c038cba-8d84-48bc-a75f-1f9baf78cbf8"}}},{"cell_type":"markdown","source":["### Classifier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bb2549b-9ae3-49be-ace3-054cbfd9fd36"}}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\nclassifier = RandomForestClassifier(featuresCol = 'features', labelCol = 'Class')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc7159a1-a8b1-462d-8c7b-fb222d54688f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Create Pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d1a2938-6641-4836-a9ef-deee40369602"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[assembler, classifier])\n\npipeline_model = pipeline.fit(finalDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cbe8688-c4be-4b32-92d8-127b803db801"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pred = pipeline_model.transform(df_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8f99a77-1753-4d8f-bef9-0072e179e350"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8253dcbc-ac9f-4a06-8147-4289160d6324"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\nbinary_evaluator = BinaryClassificationEvaluator(labelCol =\"Class\")\ntotal_time=end_time-start_time\nprint(\"Test Area Under ROC: \" + str(binary_evaluator.evaluate(pred, {binary_evaluator.metricName: \"areaUnderROC\"})))\nprint(\"Test Area Under PR: \" + str(binary_evaluator.evaluate(pred, {binary_evaluator.metricName: \"areaUnderPR\"})))\nprint(\"----------------------------------------------------------------\")\n\nevaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\")\nprint(\"Accuracy: \" + str(evaluator_accuracy.evaluate(pred)))\nprint(\"----------------------------------------------------------------\")\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\nprint(\"Precision: \" + str(evaluator_precision.evaluate(pred)))\nprint(\"----------------------------------------------------------------\")\n\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\nprint(\"Recall: \" + str(evaluator_recall.evaluate(pred)))\nprint(\"----------------------------------------------------------------\")\n\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"f1\")\nprint(\"F1: \" + str(evaluator_f1.evaluate(pred)))\nprint(\"----------------------------------------------------------------\")\n\nprint(\"Total time taken: \", total_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97f3e185-ee0b-43fc-b15a-9dcec19932b9"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Local MapReduce","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":63185267601759}},"nbformat":4,"nbformat_minor":0}
